\documentclass[11pt]{article}
\usepackage{colacl}
\usepackage{float}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\graphicspath{{img/}}
\DeclareUnicodeCharacter{F8FF}{$\diamond$}
\sloppy

\title{COMP30027 Assignment 2: Report}
\author
{Anonymous}



\begin{document}
\maketitle

\section{Introduction}\label{sec:intro}
% Introduction: a short description of the problem and data set

Sentiment analysis focuses on classifying text into sentiments.
My goal is to apply sentiment analysis to develop and train three reliable sentiment classifiers and one baseline for {T}witter posts (tweets).
This involves extracting and selecting useful features from a dataset of tweets, 
then choosing and evaluating highly reliable classifiers.

\subsection{Dataset}\label{sec:dataset}

The dataset provided contains two lists of tweets/instances made prior to 2017 \cite{dataset}.
The training set in \texttt{Train.csv} contains $21802$ labelled instances and the testing set in \texttt{Test.csv} contains $6099$ unlabelled instances. 
For each instance, included is the text and tweet ID. 
The tweets included vary in content.
For example, some are not in English: ``\textit{season in the sun versi nirvana rancak gak..slow rockkk...}''.
Tweets can either be "positive", "neutral" or "negative", distributed as shown in Figure~\ref{fig:sent-dist}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.45\textwidth]{sentiment-distribution.png}
	\caption{Training sentiment distribution}
	\label{fig:sent-dist}
\end{figure} 

\section{Methodology}
% Method: Introduce the used feature(s), and the rationale behind including them. Explain the classifiers
% and evaluation method(s) and metric(s) you have used (and why you have used them). This should be
% at a conceptual level; a detailed description of the code is not appropriate for the report. The description
% should be similar to what you would see in a machine learning conference paper.

The following methods are developed with reference to prior works on {T}witter sentiment analysis \cite{go09,nltk,robustnoisy10}.

\subsection{Instance cleaning}

Some features extracted rely on the text in the tweets being cleaned.
Cleaning involves removing stopwords (Section~\ref{sec:stopwords}), links, hashtags, mentions, numbers, non-alphanumeric characters.
Also performed is the reduction of repeated letters with more than two occurrences, as suggested by Go et al. \shortcite{go09}.

\subsubsection{Stopwords}\label{sec:stopwords}

Manual stopword list construction is tedious and inexhaustive (consider other languages). 
Instead, I start with the {P}ython Natural Language Toolkit's (\texttt{NLTK}) stopword list, which includes non-English languages \cite{nltk}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{wc/all-clean-nltk.png}
	\caption{Word cloud with \texttt{NLTK} stopword removal}
	\label{fig:wc-nltk}
\end{figure}

Next, based on Figure~\ref{fig:wc-nltk}, some extra words are manually added to the list, leading to the final vocabulary list in Figure~\ref{fig:wc-final}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{wc/all-clean-final.png}
	\caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{fig:wc-final}
\end{figure} 

\subsection{Feature extraction}

These features are all chosen for their potential correlations with sentiments.

\subsubsection{{T}witter features}

First, I extract platform-specific features from the tweets: 
hashtags, mentions, and links \cite{go09,robustnoisy10}.
These are integrated within the platform, meaning they are widely used (commonplace in tweets).

\subsubsection{Linguistic features}

Next, linguistic features are extracted and used to tokenize the tweet in different ways: 
part-of-speech tags, lemmas, lemma 2-grams, phonetics (phenomes of words), punctuation, and emoticons \cite{robustnoisy10,nltk,cmudict}.
Lemmas are preferred over words, as they group words of the same root \cite{nltk}.


Emoticons are identified as string combinations of eye, middle and mouth characters, 
using a different method to Go et al. \shortcite{go09},
since they miss many emoticons, such as the crying \texttt{:'(}.
Emoticons are then categorized into happy \texttt{:)}, sad \texttt{:(}, neutral \texttt{:|} or surprised \texttt{:o}.

\subsubsection{Metric features}

These are numeric counts of feature types within a tweet: words, characters, alphabetic characters, links, hashtags, mentions, and emoticons.
Also considered are average word length and if a tweet is quoting another.

\subsubsection{How many features per feature type is enough?}

With over 20,000 unique instances in the training dataset, many feature types generate a large number of unique features (e.g. a large vocabulary of lemmas).
Minimizing the feature space with a limit of features per feature type reduces model complexity.
To determine this number $M_f$, the top candidate classifiers are compared for each of $M_f \in \lbrace 10, 100, 1000, 5000 \rbrace$ by the accuracies found using Section~\ref{sec:choosing2}.

\subsubsection{Vectorization}

At the time that the data was collected, tweets were limited to 140 characters \cite{tweetlen}.
This means that features appear once (at most twice) in a tweet.
Therefore, non-metric features are vectorized by occurrence counts, 
as TF-IDF is not applicable with near-binary features. 

\subsection{Classifier Selection}

\subsubsection{Baseline}

The baseline model used is the 0-R: most frequent class (Classifier 4).
The three chosen models (Classifiers 1, 2, and 3) need to outperform this.

\subsubsection{Classifier Candidates}\label{sec:allcandidates}

The following classifier/parameter combinations are considered for the final three models. 

\textbf{Multinomial/Bernoulli Naive Bayes}:
\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c|c|}			
			\hline
			Parameter & Options \\
			\hline
			Include Priors & Yes, No \\
			$\alpha$ Smoothing & 0, 1, 10 \\
			\hline
		\end{tabular}
		\caption{Naive Bayes Parameters}
		\label{tbl:nb-options}
	\end{center}
\end{table}

\textbf{Logistic Regressions}:
Logistic regressions do not have values modified.
For the logistic regressions, \texttt{saga} optimization is used, as it is recommended for larger datasets \cite{skl}.

\textbf{Decision Trees}:
\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c|c|}			
			\hline
			Parameter & Options \\
			\hline
			Maximum Depth & 1, 100, 500 \\
			\hline
		\end{tabular}
		\caption{Decision Tree Parameters}
		\label{tbl:dt-options}
	\end{center}
\end{table}

\textbf{$K$ Nearest Neighbours}:
\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|}			
			\hline
			Parameter & Options \\
			\hline
			$K$ & 1, 10, 100, 500 \\
			Vote Weighting & \texttt{uniform}, \texttt{distance} \\
			\hline
		\end{tabular}
		\caption{K-Nearest Neighbour Parameters}
		\label{tbl:knn-options}
	\end{center}
\end{table}

\textbf{Support Vector Classifiers}:
\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|}			
			\hline
			Parameter & Options \\
			\hline
			Kernel & Linear, Cubic \\
			Regularization $C$ & 0.1, 1, 3 \\
			Decision Function & One-v-One, One-v-Rest \\
			\hline
		\end{tabular}
		\caption{Support Vector Classifier Parameters}
		\label{tbl:svc-options}
	\end{center}
\end{table}


\subsection{Evaluation}\label{sec:evaluations}

\subsubsection{Which Parameter Configurations Are Best?}\label{sec:choosing2}

Of the combinations in Section~\ref{sec:allcandidates}, 
the top three (Classifiers 1, 2, and 3) are determined with the following scoring system.
First, cross-validation scores are calculated on the training set.
Next, cross-validation is performed on the maximum subset of data with a uniform sentiment distribution.
The 10 scores are averaged $\bar{x}$, and their standard deviation $s$ found. 
The best candidate configurations have the highest bound for the right-sided $95\%$ confidence interval of mean cross-validation scores:
\begin{align*}
	\text{95\% C.I. Bound} &= \bar{x} - \mathbb{F}_{t_9}^{-1}(0.95) \times \frac{s}{\sqrt{n}} \\
	&= \bar{x} - 1.8331 \times \frac{s}{\sqrt{10}}
\end{align*}
where $t_9$ is the $t$-distribution with $9$ degrees of freedom and $n = 10$.

\subsubsection{Evaluation Metrics}\label{sec:evalmetrics}

Accuracies, precisions, recalls and $F_1$ scores (per sentiment) are generated for each classifier that makes it past Section~\ref{sec:choosing2}.

\subsubsection{Which models are label distribution agnostic?}

It is unknown whether the \texttt{Test.csv} and \texttt{Train.csv} sentiments are similarly distributed, 
so evaluation metrics from Section~\ref{sec:evalmetrics} are calculated on four $4:1$ train/test splits.

Where ``Given'' denotes a set of data with sentiment distributions similar to those in Figure~\ref{fig:sent-dist},
and ``Uniform'' denotes a maximum subset of data with uniformally distributed sentiments,
evaluation metrics are generated on the train/test combinations in Table~\ref{tbl:train-test}.
The metrics are then compared to determine which models are agnostic to sentiment distributions.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Training & Testing \\
			\hline
			Uniform & Uniform \\
			Given & Given \\
			Given & Uniform \\
			Uniform & Given \\
			\hline
		\end{tabular}
		\caption{Training and testing splits}
		\label{tbl:train-test}
	\end{center}
\end{table}

\section{Results}\label{sec:results}
% Results: Present the results, in terms of evaluation metric(s) and, ideally, illustrative examples and diagrams.

\subsection{Highest cross-validation accuracy per number of maximum features}\label{sec:mfresults}

After generating classifiers for each of the $M_f$ values, 
the following boasted the highest scores calculated in Section~\ref{sec:choosing2}.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Logistic Regression \\
			Parameters & 
			\begin{tabular}{l} 
				With Intercepts \& \\
				500 Max Iterations
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.5524\\
			$s$ &  0.04734 \\
			95\% C.I. Bound & 0.5250 \\
			\hline
		\end{tabular}
		\caption{$M_f = 10$ Top classifier}
		\label{tbl:mf10}
	\end{center}
\end{table}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Linear SVC \\
			Parameters & 
			\begin{tabular}{l} 
				$C = 0.1$
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.6027\\
			$s$ &  0.02625 \\
			95\% C.I. Bound & 0.5875 \\
			\hline
		\end{tabular}
		\caption{$M_f = 100$ Top classifier}
		\label{tbl:mf100}
	\end{center}
\end{table}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Support Vector Classifier \\
			Parameters & 
			\begin{tabular}{l} 
				One-v-One Decisions, \\ 
				$C = 0.1$ \& \\
				Linear Kernel
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.6415\\
			$s$ &  0.02002 \\
			95\% C.I. Bound & 0.6299 \\
			\hline
		\end{tabular}
		\caption{$M_f = 1000$ Top classifier}
		\label{tbl:mf1000}
	\end{center}
\end{table}


\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Logistic Regression \\
			Parameters & 
			\begin{tabular}{l} 
				With Intercepts \& \\
				100 Max Iterations
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.6441\\
			$s$ & 0.008317 \\
			95\% C.I. Bound & 0.6392 \\
			\hline
		\end{tabular}
		\caption{$M_f = 5000$ Top classifier}
		\label{tbl:mf5000}
	\end{center}
\end{table}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Logistic Regression \\
			Parameters & 
			\begin{tabular}{l} 
				Wihout Intercepts \& \\
				100 Max Iterations
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.6447\\
			$s$ & 0.006991 \\
			95\% C.I. Bound & 0.6406 \\
			\hline
		\end{tabular}
		\caption{$M_f = 10000$ Top classifier}
		\label{tbl:mf10000}
	\end{center}
\end{table}

These results are used to generate Figure~\ref{fig:accvsmf}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.49\textwidth]{how-mf-varies-accuracies.png}
	\caption{How $M_f$ affects the cross-validation accuracies of the highest-scoring classifiers.}
	\label{fig:accvsmf}
\end{figure} 

\subsection{Top 3 parameter/classifier configurations for $M_f = 10000$}\label{sec:top3}

The following classifiers have the highest scores calculated in Section~\ref{sec:choosing2}:

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Logistic Regression \\
			Parameters & 
			\begin{tabular}{l} 
				Wihout Intercepts \& \\
				100 Max Iterations
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.6447\\
			$s$ & 0.006991 \\
			95\% C.I. Bound & 0.6406 \\
			\hline
		\end{tabular}
		\caption{Classifier 1}
		\label{tbl:1st10000}
	\end{center}
\end{table}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Logistic Regression \\
			Parameters & 
			\begin{tabular}{l} 
				With Intercepts \& \\
				100 Max Iterations
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.6449\\
			$s$ & 0.008343 \\
			95\% C.I. Bound & 0.6401 \\
			\hline
		\end{tabular}
		\caption{Classifier 2}
		\label{tbl:2nd10000}
	\end{center}
\end{table}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Linear SVC \\
			Parameters & 
			\begin{tabular}{l} 
				One-v-One \\ 
				\& $C = 0.1$
			\end{tabular} \\
			\hline\hline
			$\bar{x}$ &  0.6491\\
			$s$ &   0.01622 \\
			95\% C.I. Bound & 0.6397 \\
			\hline
		\end{tabular}
		\caption{Classifier 3}
		\label{tbl:3rd10000}
	\end{center}
\end{table}

\subsection{Evaluation metrics for the final 4 models}\label{sec:4metrics}

The metrics describes in Section~\ref{sec:evalmetrics} are calculated for the final classifiers (Classifiers 1, 2, 3, and 4). 
Per-class metrics are shown in order of postive (+), neutral (n), then negative (--) sentiments.
% See Section~\ref{sec:appa} (Appendix).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsubsection{Results for Classifier 1}\label{sec:appc1}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Logistic Regression \\
			Parameters & 
			\begin{tabular}{l} 
				Without Intercepts \& \\
				100 Max Iterations
			\end{tabular} \\
			\hline\hline
			Uniform Training & Uniform Testing \\
			\hline
			Accuracy & 0.64 \\
			Precisions (+, n, --)	& 0.69, 0.54, 0.69 \\
			Recalls (+, n, --) 		& 0.68, 0.55, 0.69 \\
			$F_1$ Scores (+, n, --) & 0.69, 0.54, 0.69 \\
			\hline\hline
			Given Training & Given Testing \\
			\hline
			Accuracy & 0.65 \\
			Precisions (+, n, --) 	& 0.69, 0.65, 0.53 \\
			Recalls (+, n, --) 		& 0.40, 0.89, 0.18 \\
			$F_1$ Scores (+, n, --) & 0.51, 0.75, 0.27 \\
			\hline\hline
			Given Training & Uniform Testing \\
			\hline
			Accuracy & 0.54 \\
			Precisions (+, n, --) 	& 0.81, 0.43, 0.82 \\
			Recalls (+, n, --) 		& 0.48, 0.91, 0.24 \\
			$F_1$ Scores (+, n, --) & 0.60, 0.58, 0.37 \\
			\hline\hline
			Uniform Training & Given Testing \\
			\hline
			Accuracy & 0.73 \\
			Precisions (+, n, --) 	& 0.64, 0.91, 0.57 \\
			Recalls (+, n, --) 		& 0.84, 0.62, 0.93 \\
			$F_1$ Scores (+, n, --) & 0.73, 0.74, 0.71 \\
			\hline
		\end{tabular}
		\caption{Classifier 1 metrics}
		\label{tbl:metrics-1st10000}
	\end{center}
\end{table}

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-1st-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for given training and given testing data}
% 	\label{fig:cf-1st-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-1st-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-1st-ur}
% \end{figure} 

% \pagebreak
% \subsubsection{Results for Classifier 2}\label{sec:appc2}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Logistic Regression \\
			Parameters & 
			\begin{tabular}{l} 
				With Intercepts \& \\
				100 Max Iterations
			\end{tabular} \\
			\hline\hline
			Uniform Training & Uniform Testing \\
			\hline
			Accuracy & 0.64 \\
			Precisions (+, n, --) 	& 0.69, 0.54, 0.69 \\
			Recalls (+, n, --) 		& 0.68, 0.54, 0.69 \\
			$F_1$ Scores (+, n, --) & 0.68, 0.54, 0.69 \\
			\hline\hline
			Given Training & Given Testing \\
			\hline
			Accuracy & 0.64 \\
			Precisions (+, n, --) 	& 0.68, 0.65, 0.52 \\
			Recalls (+, n, --) 		& 0.40, 0.89, 0.17 \\
			$F_1$ Scores (+, n, --) & 0.50, 0.75, 0.26 \\
			\hline\hline
			Given Training & Uniform Testing \\
			\hline
			Accuracy & 0.54 \\
			Precisions (+, n, --) 	& 0.81, 0.43, 0.82 \\
			Recalls (+, n, --) 		& 0.48, 0.91, 0.23 \\
			$F_1$ Scores (+, n, --) & 0.60, 0.58, 0.36 \\
			\hline\hline
			Uniform Training & Given Testing \\
			\hline
			Accuracy & 0.73 \\
			Precisions (+, n, --) 	& 0.64, 0.91, 0.57 \\
			Recalls (+, n, --) 		& 0.84, 0.62, 0.93 \\
			$F_1$ Scores (+, n, --) & 0.73, 0.74, 0.71 \\
			\hline
		\end{tabular}
		\caption{Classifer 2 metrics}
		\label{tbl:metrics-2nd10000}
	\end{center}
\end{table}

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Uniform-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-2nd-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Given-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for given training and given testing data}
% 	\label{fig:cf-2nd-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-2nd-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-2nd-ur}
% \end{figure} 

% \pagebreak
% \subsubsection{Results for Classifier 3}\label{sec:appc3}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Classifier & Linear SVC \\
			Parameters & 
			\begin{tabular}{l} 
				One-v-One \\ 
				\& $C = 0.1$
			\end{tabular} \\
			\hline\hline
			Uniform Training & Uniform Testing \\
			\hline
			Accuracy & 0.64 \\
			Precisions (+, n, --) 	& 0.70, 0.53, 0.68 \\
			Recalls (+, n, --) 		& 0.67, 0.55, 0.69 \\
			$F_1$ Scores (+, n, --) & 0.69, 0.54, 0.69 \\
			\hline\hline
			Given Training & Given Testing \\
			\hline
			Accuracy & 0.65 \\
			Precisions (+, n, --) 	& 0.66, 0.66, 0.54 \\
			Recalls (+, n, --) 		& 0.46, 0.85, 0.26 \\
			$F_1$ Scores (+, n, --) & 0.54, 0.74, 0.35 \\
			\hline\hline
			Given Training & Uniform Testing \\
			\hline
			Accuracy & 0.65 \\
			Precisions (+, n, --) 	& 0.85, 0.50, 0.92 \\
			Recalls (+, n, --) 		& 0.61, 0.91, 0.43 \\
			$F_1$ Scores (+, n, --) & 0.71, 0.65, 0.58 \\
			\hline\hline
			Uniform Training & Given Testing \\
			\hline
			Accuracy & 0.72 \\
			Precisions (+, n, --) 	& 0.66, 0.89, 0.55 \\
			Recalls (+, n, --) 		& 0.82, 0.62, 0.92 \\
			$F_1$ Scores (+, n, --) & 0.73, 0.74, 0.69 \\
			\hline
		\end{tabular}
		\caption{Classifer 3 metrics}
		\label{tbl:metrics-3rd10000}
	\end{center}
\end{table}

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-3rd-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for given training and given testing data}
% 	\label{fig:cf-3rd-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-3rd-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-3rd-ur}
% \end{figure}

% \pagebreak
% \subsubsection{Results for the baseline model (Classifier 4)}\label{sec:appc4}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			% Parameter & Value \\
			% \hline
			% Classifier & 0-R \\
			Classifier & 0-R \\
			\hline\hline
			Uniform Training & Uniform Testing \\
			\hline
			Accuracy & 0.33 \\
			Precisions (+, n, --) 	& 0.00, 0.00, 0.33 \\
			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.50 \\
			\hline\hline
			Given Training & Given Testing \\
			\hline
			Accuracy & 0.58 \\
			Precisions (+, n, --) 	& 0.00, 0.58, 0.00 \\
			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
			$F_1$ Scores (+, n, --) & 0.00, 0.73, 0.00 \\
			\hline\hline
			Given Training & Uniform Testing \\
			\hline
			Accuracy & 0.33 \\
			Precisions (+, n, --) 	& 0.00, 0.33, 0.00 \\
			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
			$F_1$ Scores (+, n, --) & 0.00, 0.50, 0.00 \\
			\hline\hline
			Uniform Training & Given Testing \\
			\hline
			Accuracy & 0.17 \\
			Precisions (+, n, --) 	& 0.00, 0.00, 0.17 \\
			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.29 \\
			\hline
		\end{tabular}
		\caption{Baseline (Classifier 4) metrics}
		\label{tbl:metrics-base10000}
	\end{center}
\end{table}

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Uniform-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-base-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Given-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for given training and given testing data}
% 	\label{fig:cf-base-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-base-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-base-ur}
% \end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis}
% Discussion / Critical Analysis: Contextualise the systems' behaviour, based on the understanding of the
% subject materials (This is the most important part of the task in this assignment).

% Contextualise implies that we are more interested in seeing evidence of you have thought about the task
% and determining reasons for the relative performance of different methods, rather than the raw scores of
% the different methods you selected. This is not to say that you should ignore the relative performance of
% different runs over the data, but rather that you should think beyond simple numbers to the reasons that
% underlie them.

\subsection{How $M_f$ modifies the models}\label{sec:mfanalysis}


It appears that $\emph{accuracy} \varpropto M_f$.
The number of maximum features $M_f$ does affect the accuracy of models, as shown in Figure~\ref{fig:accvsmf}.
As $M_f$ increases, the accuracy of the top model increases as well, albeit asymptotically.
This confirms that a larger feature space allows for more accurate results to a limit.
With 10 features per feature type, the odds of running into an instance without any of these is higher. 
As the feature space increases in size, more features can be considered when classifying a new instance, 
meaning more instances are likely to contain at least one feature to consider for classification.
Past $M_f = 5000$, the gains in accuracy become negligible.
Despite this, the accuracy does increase, 
leading to the use of $M_f = 10000$ to generate and evalute the final models.


The average accuracy for cross-validation scores over the splits with the given sentiment distributions does not follow the same relationship to $M_f$ as the other accuracy scores.
This could potentially suggest that past $M_f = 1000$, training or testing on the given sentiments is unadvisable.



The modal top classifier within this test is the logistic regression.
These results highlight the model's versatility with different sizes of the feature space.


One outlier is the linear support vector classifier at $M_f = 1000$. 
This confirms that selecting the final models required deeper investigation into classifier results than simple cross-validation.
Such analysis occurs in Sections~\ref{sec:bestparams} and \ref{sec:agnostic}.

\subsection{The best classifier/parameter configurations}\label{sec:bestparams}

% The best classifier configurations with a large feature space are naive Bayes estimators, as shown in Section~\ref{sec:top3}.
% The highest scoring model is the Bernoulli naive Bayes, with $\alpha = 1$ laplace smoothing and sentiment/class priors included.

Of the classifiers evaluated at $M_f = 10000$, 
the top three models (shown in Tables~\ref{tbl:1st10000}, \ref{tbl:2nd10000}, and \ref{tbl:3rd10000}) are comprised of two logistic regressions and a linear support vector classifier.
Interestingly, the Bernoulli naive Bayes classifiers do not appear in the top classifier list.
The nature of the features and tweet lengths means that almost all the features are binary events (Bernoulli trials),
making the naive Bayes classifier very applicable \cite{tweetlen}. 
On the other hand, logistic regressions by nature are also well suited to binary features,
and have improved performance through the use of optimization algorithms.


The Classifiers 1, 2, and 3 all perform better than the baseline model (see Section~\ref{sec:4metrics}). 
The only exception is in the recalls of the 0-R, in which the baseline model has the best results.
However, this is misleading since the 0-R baseline classifier labels all instances with the same class, 
guarranteeing that one of the classes has no false negative values.


\subsection{The most sentiment distribution agnostic classifier}\label{sec:agnostic}

Since the \texttt{Test.csv} and \texttt{Train.csv} data are not guarranteed to follow the same sentiment distributions,
the true best model is selected by comparing $F_1$ scores.
As shown in Section~\ref{sec:4metrics}, the sentiment distribution of the training and testing data influences the evaluation metrics of classifiers.


From the accuracies of the top 3 models on different distribution splits (Tables~\ref{tbl:metrics-1st10000}, \ref{tbl:metrics-2nd10000}, and \ref{tbl:metrics-3rd10000}), 
training on given data and testing on uniform data yields consistently high precisions and recalls for all sentiments.
This is the opposite for training on given data distributions, where precisions are higher than recalls for non-neutral sentiments, 
suggesting that confirming a non-neutral sentiment at the point of classification proves difficult.


Thus, training on the data with the given sentiment distribution is not preferrable if the true testing data does not follow the same sentiment distribution.
In fact, training on uniform data generally yields better accuracy and higher $F_1$ scores, 
even if the distribution of sentiments in the test set is not uniform.
Therefore, the best-performing models non-neutral sentiments are trained on the maximum uniform subset instead of the full dataset.


The only hyperparameter that differs between Classifier 1 and Classifier 2 is inclusion of intercept terms in the regression.
The inclusion of intercepts in the regression models can bias the models towards different sentiments.
To measure this bias, the 
% To consider how the hyperparameter affects the model, 
% high precision and high recall (or simply the $F_1$ Scores) for non-neutral classes are compared, 
% since neutral sentiment is common in the training set.
% Fitting the prior labels reduces the accuracy when training on the given set and testing on the uniform set.
% This is a small difference of $0.01$ in accuracy. 
% However, it confirms that if the \texttt{Test.csv} dataset has a different sentiment distribution to the \texttt{Train.csv}, 
% it is preferrable to prevent prior distributions from influencing predictions.


\section{Conclusions}
% Conclusion: Demonstrate your identified knowledge about the problem.
Performing sentiment analysis on tweets is a difficult task.
Vectorization of the features yields a large feature space, which contributes to the time and space complexity in training classifiers.
While minimizing the feature space reduces complexity, it also sacrifices the accuracy of classifiers (some more than others).

Testing a multitude of different classifier combinations, the highest scoring (calculated by Section~\ref{sec:choosing2}) classifiers are naive Bayes classifiers, 
with the best model relying specifically on a Bernoulli naive Bayes classifier.
Bernoulli models make sense in this case, given the small sizes of tweets meaning features appear no more than once in an instance \cite{tweetlen}.
With little information on the \texttt{Test.csv} set, 
the models generated need to be highly accurate regardless of the distributions of the training sentiments.

\textbf{Of the classifiers considered, the most efficacious, 
is a Bernoulli naive Bayes model trained on the maximum subset of data with uniform sentiment distribution with laplace smoothing $\alpha = 1$, 
without fitting prior distributions.}

\pagebreak
\bibliographystyle{acl}
\bibliography{citations}

% \pagebreak
% \listoftables
% \section{Appendix: Evaluation of top classifiers for $M_f = 10000$}\label{sec:appa}

% \subsection{Results for Classifier 1}\label{sec:appc1}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & Bernoulli N.B. \\
% 			Include Priors & Yes \\
% 			$\alpha$ Smoothing & 1 \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.62 \\
% 			Precisions (+, n, --)	& 0.65, 0.55, 0.65 \\
% 			Recalls (+, n, --) 		& 0.70, 0.44, 0.74 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.49, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.60 \\
% 			Precisions (+, n, --) 	& 0.78, 0.60, 0.50 \\
% 			Recalls (+, n, --) 		& 0.14, 0.99, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.23, 0.74, 0.01 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.40 \\
% 			Precisions (+, n, --) 	& 0.94, 0.36, 0.90 \\
% 			Recalls (+, n, --) 		& 0.19, 0.99, 0.01 \\
% 			$F_1$ Scores (+, n, --) & 0.31, 0.52, 0.02 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.64 \\
% 			Precisions (+, n, --) 	& 0.56, 0.87, 0.48 \\
% 			Recalls (+, n, --) 		& 0.82, 0.50, 0.85 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.64, 0.62 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifier 1 metrics}
% 		\label{tbl:metrics-1st10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-1st-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-1st-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-1st-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-1st-ur}
% % \end{figure} 

% \pagebreak
% \subsection{Results for Classifier 2}\label{sec:appc2}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & Bernoulli N.B. \\
% 			Include Priors & No \\
% 			$\alpha$ Smoothing & 1 \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.62 \\
% 			Precisions (+, n, --) 	& 0.65, 0.55, 0.65 \\
% 			Recalls (+, n, --) 		& 0.70, 0.44, 0.74 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.49, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.61 \\
% 			Precisions (+, n, --) 	& 0.73, 0.60, 0.60 \\
% 			Recalls (+, n, --) 		& 0.16, 0.98, 0.01 \\
% 			$F_1$ Scores (+, n, --) & 0.26, 0.74, 0.02 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.42 \\
% 			Precisions (+, n, --) 	& 0.93, 0.36, 0.95 \\
% 			Recalls (+, n, --) 		& 0.23, 0.99, 0.03 \\
% 			$F_1$ Scores (+, n, --) & 0.37, 0.53, 0.05 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.64 \\
% 			Precisions (+, n, --) 	& 0.56, 0.87, 0.48 \\
% 			Recalls (+, n, --) 		& 0.82, 0.50, 0.85 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.64, 0.62 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifer 2 metrics}
% 		\label{tbl:metrics-2nd10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Uniform-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-2nd-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Given-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-2nd-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-2nd-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-2nd-ur}
% % \end{figure} 

% \pagebreak
% \subsection{Results for Classifier 3}\label{sec:appc3}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & Multinomial N.B. \\
% 			Include Priors & Yes \\
% 			$\alpha$ Smoothing & 1 \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.62 \\
% 			Precisions (+, n, --) 	& 0.67, 0.55, 0.62 \\
% 			Recalls (+, n, --) 		& 0.67, 0.41, 0.78 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.47, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.61 \\
% 			Precisions (+, n, --) 	& 0.79, 0.60, 0.20 \\
% 			Recalls (+, n, --) 		& 0.17, 0.98, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.28, 0.74, 0.01 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.42 \\
% 			Precisions (+, n, --) 	& 0.94, 0.36, 1.00 \\
% 			Recalls (+, n, --) 		& 0.26, 0.99, 0.02 \\
% 			$F_1$ Scores (+, n, --) & 0.41, 0.53, 0.04 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.63 \\
% 			Precisions (+, n, --) 	& 0.58, 0.88, 0.45 \\
% 			Recalls (+, n, --) 		& 0.79, 0.48, 0.89 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.62, 0.60 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifer 3 metrics}
% 		\label{tbl:metrics-3rd10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-3rd-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-3rd-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-3rd-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-3rd-ur}
% % \end{figure}

% \pagebreak
% \subsection{Results for the baseline model (Classifier 4)}\label{sec:appc4}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & 0-R \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.33 \\
% 			Precisions (+, n, --) 	& 0.00, 0.00, 0.33 \\
% 			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.50 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.58 \\
% 			Precisions (+, n, --) 	& 0.00, 0.58, 0.00 \\
% 			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.73, 0.00 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.33 \\
% 			Precisions (+, n, --) 	& 0.00, 0.33, 0.00 \\
% 			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.50, 0.00 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.17 \\
% 			Precisions (+, n, --) 	& 0.00, 0.00, 0.17 \\
% 			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.29 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Baseline classifier (Classifier 4) metrics}
% 		\label{tbl:metrics-base10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Uniform-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-base-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Given-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-base-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-base-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-base-ur}
% % \end{figure} 

\end{document}
