\documentclass[11pt]{article}
\usepackage{colacl}
\usepackage{float}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\graphicspath{{img/}}
\DeclareUnicodeCharacter{F8FF}{$\diamond$}
\sloppy

\title{COMP30027 Assignment 2: Report}
\author
{Anonymous}



\begin{document}
\maketitle

\section{Introduction}\label{sec:intro}
% Introduction: a short description of the problem and data set

Sentiment analysis focuses on classifying text into sentiments.
My goal is to apply sentiment analysis to develop and train three reliable sentiment classifiers and one baseline for {T}witter posts (tweets).
This involves extracting and selecting useful features from a dataset of tweets, 
then choosing and evaluating highly reliable classifiers.

\subsection{Dataset}\label{sec:dataset}

The dataset provided contains two lists of tweets/instances made prior to 2017 \cite{dataset}.
The training set in \texttt{Train.csv} contains $21802$ labelled instances and the testing set in \texttt{Test.csv} contains $6099$ unlabelled instances. 
For each instance, included is the text and tweet ID. 
The tweets included vary in content.
For example, some are not in English: ``\textit{season in the sun versi nirvana rancak gak..slow rockkk...}''.
Tweets can either be "positive", "neutral" or "negative", distributed as shown in Figure~\ref{fig:sent-dist}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.45\textwidth]{sentiment-distribution.png}
	\caption{Training sentiment distribution}
	\label{fig:sent-dist}
\end{figure} 

\section{Methodology}
% Method: Introduce the used feature(s), and the rationale behind including them. Explain the classifiers
% and evaluation method(s) and metric(s) you have used (and why you have used them). This should be
% at a conceptual level; a detailed description of the code is not appropriate for the report. The description
% should be similar to what you would see in a machine learning conference paper.

The following methods are developed with reference to prior works on {T}witter sentiment analysis \cite{go09,nltk,robustnoisy10}.

\subsection{Instance cleaning}

Some features extracted rely on the text in the tweets being cleaned.
Cleaning involves removing stopwords (Section~\ref{sec:stopwords}), links, hashtags, mentions, numbers, non-alphanumeric characters.
Also performed is the reduction of repeated letters with more than two occurrences, as suggested by Go et al. \shortcite{go09}.

\subsubsection{Stopwords}\label{sec:stopwords}

Manual stopword list construction omits stopwords from other languages. 
Instead, I start with the {P}ython Natural Language Toolkit's (\texttt{NLTK}) stopword list, which includes non-English languages \cite{nltk}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{wc/all-clean-nltk.png}
	\caption{Word cloud with \texttt{NLTK} stopword removal}
	\label{fig:wc-nltk}
\end{figure}

Next, referring to Figure~\ref{fig:wc-nltk}, some extra words are manually added to the final stopword list, 
leading to the final training vocabulary in Figure~\ref{fig:wc-final}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{wc/all-clean-final.png}
	\caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{fig:wc-final}
\end{figure} 

\subsection{Feature extraction}

These features are all chosen for their potential correlations with sentiments.

\subsubsection{{T}witter features}

First, I extract platform-specific features from the tweets: 
hashtags, mentions, and links \cite{go09,robustnoisy10}.
These are integrated within the platform, meaning they are widely used (commonplace in tweets).

\subsubsection{Linguistic features}

Next, linguistic features are extracted and used to tokenize the tweet in different ways: 
part-of-speech tags, lemmas, lemma 2-grams, phonetics (phonemes of words), punctuation, and emoticons \cite{robustnoisy10,nltk,cmudict}.
Lemmas are preferred over words, as they group words of the same root \cite{nltk}.


Emoticons are identified as string combinations of eye, middle and mouth characters, 
using a different method to Go et al. \shortcite{go09},
since they miss many emoticons, such as the crying \texttt{:'(}.
Emoticons are then categorized into happy \texttt{:)}, sad \texttt{:(}, neutral \texttt{:|} or surprised \texttt{:o}.

\subsubsection{Binary features}

Binary features track the appearance of other feature types within a tweet: 
whether the tweet contains quotes, a hashtag, mention, link, or emoticons.
These binary values may correlate to non-neutral sentiments.

\subsubsection{How many features per feature type is enough?}

With over 20,000 unique instances in the training dataset, many feature types generate a large number of unique features (e.g. a large vocabulary of lemmas).
Minimizing the feature space with a limit of features per feature type reduces model complexity.
To determine this number $M_f$, the top candidate classifiers are compared for each of $M_f \in \lbrace 10, 100, 1000, 5000, 10000\rbrace$ by the accuracies found using Section~\ref{sec:choosing2}.

\subsubsection{Vectorization}\label{sec:vectorization}

At the time that the data was collected, tweets were limited to 140 characters \cite{tweetlen}.
This means that features appear once (at most twice) in a tweet.
Therefore, non-metric features are vectorized by occurrence counts, 
as TF-IDF is not applicable with near-binary features. 

\subsection{Classifier Selection}

\subsubsection{Baseline}

The baseline model used is the 0-R: most frequent class (Classifier 4).
The three chosen models (Classifiers 1, 2, and 3) need to outperform this.

\subsubsection{Classifier Candidates}\label{sec:allcandidates}

The following classifier/parameter combinations are considered for the final three models. 

\textbf{Multinomial/Bernoulli Naive Bayes}:
% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|c|c|}			
% 			\hline
% 			Parameter & Options \\
% 			\hline
% 			Include Priors & Yes, No \\
% 			$\alpha$ Smoothing & 0, 1, 10 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Naive Bayes Parameters}
% 		\label{tbl:nb-options}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{tb nb.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:nb-options}
\end{figure} 



\textbf{Logistic Regressions}:
Logistic regressions do not have parameters modified.
For the logistic regressions, \texttt{saga} optimization is used, as it is recommended for larger datasets \cite{skl}.

\textbf{Decision Trees}:
% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|c|c|}			
% 			\hline
% 			Parameter & Options \\
% 			\hline
% 			Maximum Depth & 1, 100, 500 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Decision Tree Parameters}
% 		\label{tbl:dt-options}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{tb dt.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:dt-options}
\end{figure} 

\textbf{$K$ Nearest Neighbours}:
% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|c|c|}			
% 			\hline
% 			Parameter & Options \\
% 			\hline
% 			$K$ & 1, 10, 100, 500 \\
% 			Vote Weighting & \texttt{uniform}, \texttt{distance} \\
% 			\hline
% 		\end{tabular}
% 		\caption{K-Nearest Neighbour Parameters}
% 		\label{tbl:knn-options}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{tb knn.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:knn-options}
\end{figure} 


\pagebreak
\textbf{Linear Support Vector Classifiers}:
% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|c|c|}			
% 			\hline
% 			Parameter & Options \\
% 			\hline
% 			Regularization $C$ & 0.1, 1, 3 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Linear Support Vector Classifier Parameters}
% 		\label{tbl:svc-options}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{tb svc.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:svc-options}
\end{figure} 


\subsection{Evaluation}\label{sec:evaluations}

\subsubsection{Which Parameter Configurations Are Best?}\label{sec:choosing2}

Of the combinations in Section~\ref{sec:allcandidates}, 
the top three (Classifiers 1, 2, and 3) are determined with the following scoring system.
Cross-validation scores are calculated on the training set with 10 folds.
10 folds allow for a random assortment of large subsets to be scored .
The 10 scores are averaged $\bar{x}$, and their standard deviation $s$ found. 
The best candidate configurations have the highest bound for the right-sided $95\%$ confidence interval of mean cross-validation scores (5th percentile accuracy):
\begin{align*}
	\text{95\% C.I. Bound} &= \bar{x} - \mathbb{F}_{t_9}^{-1}(0.95) \times \frac{s}{\sqrt{n}} \\
	&= \bar{x} - 1.8331 \times \frac{s}{\sqrt{10}}
\end{align*}
where $t_9$ is the $t$-distribution with $9$ degrees of freedom and $n = 10$.

\subsubsection{Evaluation Metrics}\label{sec:evalmetrics}

Accuracies, precisions, recalls and $F_1$ scores (per sentiment) are generated for each classifier that makes it past Section~\ref{sec:choosing2}.

\subsubsection{Which models are label distribution agnostic?}

It is unknown whether the \texttt{Test.csv} and \texttt{Train.csv} sentiments are similarly distributed, 
so evaluation metrics from Section~\ref{sec:evalmetrics} are calculated on four $4:1$ train/test splits.

Where ``Given'' denotes a set of data with sentiment distributions similar to those in Figure~\ref{fig:sent-dist},
and ``Uniform'' denotes a maximum subset of data with uniformly distributed sentiments,
evaluation metrics are generated on the train/test combinations in Table~\ref{tbl:train-test}.
The metrics are then compared to determine which models are agnostic to sentiment distributions.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|l|}			
			\hline
			Training & Testing \\
			\hline
			Uniform & Uniform \\
			Given & Given \\
			Given & Uniform \\
			Uniform & Given \\
			\hline
		\end{tabular}
		\caption{Training and testing splits}
		\label{tbl:train-test}
	\end{center}
\end{table}

\section{Results}\label{sec:results}
% Results: Present the results, in terms of evaluation metric(s) and, ideally, illustrative examples and diagrams.

\subsection{Highest cross-validation accuracy per number of maximum features}\label{sec:mfresults}

After generating classifiers for each of the $M_f$ values, 
the following boasted the highest scores calculated in Section~\ref{sec:choosing2}.

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Logistic Regression \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6009\\
% 			$s$ &  0.03539 \\
% 			95\% C.I. Bound & 0.5804 \\
% 			\hline
% 		\end{tabular}
% 		\caption{$M_f = 10$ Top classifier}
% 		\label{tbl:mf10}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{mf10.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:mf10}
\end{figure} 

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Logistic Regression \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6334\\
% 			$s$ &  0.01925 \\
% 			95\% C.I. Bound & 0.6222 \\
% 			\hline
% 		\end{tabular}
% 		\caption{$M_f = 100$ Top classifier}
% 		\label{tbl:mf100}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{mf100.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:mf100}
\end{figure} 

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Logistic Regression \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6599\\
% 			$s$ &  0.008548 \\
% 			95\% C.I. Bound & 0.6549 \\
% 			\hline
% 		\end{tabular}
% 		\caption{$M_f = 1000$ Top classifier}
% 		\label{tbl:mf1000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{mf1000.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:mf1000}
\end{figure} 


% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Logistic Regression \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6662\\
% 			$s$ & 0.006952 \\
% 			95\% C.I. Bound & 0.6621 \\
% 			\hline
% 		\end{tabular}
% 		\caption{$M_f = 5000$ Top classifier}
% 		\label{tbl:mf5000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{mf5000.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:mf5000}
\end{figure} 


% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Logistic Regression \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6676\\
% 			$s$ & 0.006673 \\
% 			95\% C.I. Bound & 0.6637 \\
% 			\hline
% 		\end{tabular}
% 		\caption{$M_f = 10000$ Top classifier}
% 		\label{tbl:mf10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{mf10000.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:mf10000}
\end{figure} 

These results are used to generate Figure~\ref{fig:accvsmf}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.49\textwidth]{how-mf-varies-accuracies.png}
	\caption{How $M_f$ affects the cross-validation accuracies of the highest-scoring classifiers.}
	\label{fig:accvsmf}
\end{figure} 

\subsection{Top 3 parameter/classifier configurations for $M_f = 10000$}\label{sec:top3}

The following classifiers have the highest scores calculated in Section~\ref{sec:choosing2}:

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Logistic Regression \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6676\\
% 			$s$ & 0.006673 \\
% 			95\% C.I. Bound & 0.6637 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifier 1}
% 		\label{tbl:1st10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{c1s.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:1st10000}
\end{figure} 

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Linear SVC \\
% 			Parameters & 
% 			\begin{tabular}{l} 
% 				$C = 0.1$
% 			\end{tabular} \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6615\\
% 			$s$ & 0.005878 \\
% 			95\% C.I. Bound & 0.6581 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifier 2}
% 		\label{tbl:2nd10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{c2s.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:1nd10000}
\end{figure} 

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Multinomial Naive Bayes \\
% 			Parameters & 
% 			\begin{tabular}{l} 
% 				Without Priors \\ 
% 				\& $\alpha = 1$
% 			\end{tabular} \\
% 			\hline\hline
% 			$\bar{x}$ &  0.6434\\
% 			$s$ &   0.006020 \\
% 			95\% C.I. Bound & 0.6399 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifier 3}
% 		\label{tbl:3rd10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{c3s.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:3rd10000}
\end{figure} 

\subsection{Evaluation metrics for the final 4 models}\label{sec:4metrics}

The metrics describes in Section~\ref{sec:evalmetrics} are calculated for the final classifiers (Classifiers 1, 2, 3, and 4). 
Per-class metrics are shown in order of postive (+), neutral (n), then negative (--) sentiments.
% See Section~\ref{sec:appa} (Appendix).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsubsection{Results for Classifier 1}\label{sec:appc1}

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Logistic Regression \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.63 \\
% 			Precisions (+, n, --)	& 0.68, 0.53, 0.68 \\
% 			Recalls (+, n, --) 		& 0.66, 0.54, 0.69 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.54, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.66 \\
% 			Precisions (+, n, --) 	& 0.64, 0.67, 0.56 \\
% 			Recalls (+, n, --) 		& 0.49, 0.84, 0.29 \\
% 			$F_1$ Scores (+, n, --) & 0.55, 0.74, 0.38 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.65 \\
% 			Precisions (+, n, --) 	& 0.84, 0.50, 0.88 \\
% 			Recalls (+, n, --) 		& 0.60, 0.90, 0.44 \\
% 			$F_1$ Scores (+, n, --) & 0.70, 0.65, 0.59 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.75 \\
% 			Precisions (+, n, --) 	& 0.66, 0.92, 0.60 \\
% 			Recalls (+, n, --) 		& 0.85, 0.64, 0.96 \\
% 			$F_1$ Scores (+, n, --) & 0.74, 0.76, 0.74 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifier 1 metrics}
% 		\label{tbl:metrics-1st10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{c1m.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:metrics-1st10000}
\end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-1st-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for given training and given testing data}
% 	\label{fig:cf-1st-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-1st-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Classifier 1's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-1st-ur}
% \end{figure} 

% \pagebreak
% \subsubsection{Results for Classifier 2}\label{sec:appc2}

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Linear SVC \\
% 			Parameters & 
% 			\begin{tabular}{l} 
% 				$C = 0.1$
% 			\end{tabular} \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.64 \\
% 			Precisions (+, n, --) 	& 0.67, 0.54, 0.68 \\
% 			Recalls (+, n, --) 		& 0.68, 0.52, 0.71 \\
% 			$F_1$ Scores (+, n, --) & 0.68, 0.53, 0.70 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.65 \\
% 			Precisions (+, n, --) 	& 0.60, 0.68, 0.56 \\
% 			Recalls (+, n, --) 		& 0.52, 0.80, 0.31 \\
% 			$F_1$ Scores (+, n, --) & 0.56, 0.73, 0.40 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.73 \\
% 			Precisions (+, n, --) 	& 0.85, 0.58, 0.92 \\
% 			Recalls (+, n, --) 		& 0.71, 0.89, 0.58 \\
% 			$F_1$ Scores (+, n, --) & 0.77, 0.70, 0.72 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.74 \\
% 			Precisions (+, n, --) 	& 0.65, 0.92, 0.59 \\
% 			Recalls (+, n, --) 		& 0.86, 0.63, 0.96 \\
% 			$F_1$ Scores (+, n, --) & 0.74, 0.75, 0.73 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifer 2 metrics}
% 		\label{tbl:metrics-2nd10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{c2m.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:metrics-2nd10000}
\end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Uniform-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-2nd-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Given-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for given training and given testing data}
% 	\label{fig:cf-2nd-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-2nd-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Classifier 2's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-2nd-ur}
% \end{figure} 

% \pagebreak
% \subsubsection{Results for Classifier 3}\label{sec:appc3}

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Classifier & Multinomial NB \\
% 			Parameters & 
% 			\begin{tabular}{l} 
% 				Without Priors \\ 
% 				\& $\alpha = 1$
% 			\end{tabular} \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.62 \\
% 			Precisions (+, n, --) 	& 0.65, 0.56, 0.63 \\
% 			Recalls (+, n, --) 		& 0.71, 0.40, 0.75 \\
% 			$F_1$ Scores (+, n, --) & 0.68, 0.47, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.63 \\
% 			Precisions (+, n, --) 	& 0.65, 0.63, 0.59 \\
% 			Recalls (+, n, --) 		& 0.35, 0.91, 0.10 \\
% 			$F_1$ Scores (+, n, --) & 0.46, 0.75, 0.16 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.59 \\
% 			Precisions (+, n, --) 	& 0.84, 0.45, 0.95 \\
% 			Recalls (+, n, --) 		& 0.52, 0.93, 0.31 \\
% 			$F_1$ Scores (+, n, --) & 0.65, 0.61, 0.46 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.64 \\
% 			Precisions (+, n, --) 	& 0.56, 0.89, 0.48 \\
% 			Recalls (+, n, --) 		& 0.83, 0.48, 0.89 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.63, 0.63 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifer 3 metrics}
% 		\label{tbl:metrics-3rd10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{c3m.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:metrics-3rd10000}
\end{figure} 


% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-3rd-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for given training and given testing data}
% 	\label{fig:cf-3rd-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-3rd-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Classifier 3's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-3rd-ur}
% \end{figure}

% \pagebreak
% \subsubsection{Results for the baseline model (Classifier 4)}\label{sec:appc4}

% \begin{table}[H]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			% Parameter & Value \\
% 			% \hline
% 			% Classifier & 0-R \\
% 			Classifier & 0-R \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.33 \\
% 			Precisions (+, n, --) 	& 0.00, 0.00, 0.33 \\
% 			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.50 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.58 \\
% 			Precisions (+, n, --) 	& 0.00, 0.58, 0.00 \\
% 			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.73, 0.00 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.33 \\
% 			Precisions (+, n, --) 	& 0.00, 0.33, 0.00 \\
% 			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.50, 0.00 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.17 \\
% 			Precisions (+, n, --) 	& 0.00, 0.00, 0.17 \\
% 			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.29 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Baseline (Classifier 4) metrics}
% 		\label{tbl:metrics-base10000}
% 	\end{center}
% \end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.4\textwidth]{c4m.png}
	% \caption{Word cloud with modified \texttt{NLTK} stopword removal}
	\label{tbl:metrics-base10000}
\end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Uniform-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for uniform training and uniform testing data}
% 	\label{fig:cf-base-uu}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Given-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for given training and given testing data}
% 	\label{fig:cf-base-rr}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-GivenTrainingandUniformTesting-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for given training and uniform testing data}
% 	\label{fig:cf-base-ru}
% \end{figure} 

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-UniformTrainingandGivenTesting-confusion-matrix.png}
% 	\caption{Baseline classifier's confusion matrix for uniform training and given testing data}
% 	\label{fig:cf-base-ur}
% \end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis}
% Discussion / Critical Analysis: Contextualise the systems' behaviour, based on the understanding of the
% subject materials (This is the most important part of the task in this assignment).

% Contextualise implies that we are more interested in seeing evidence of you have thought about the task
% and determining reasons for the relative performance of different methods, rather than the raw scores of
% the different methods you selected. This is not to say that you should ignore the relative performance of
% different runs over the data, but rather that you should think beyond simple numbers to the reasons that
% underlie them.

\subsection{How $M_f$ modifies the models}\label{sec:mfanalysis}

It appears that $\emph{accuracy} \varpropto M_f$.
The number of maximum features $M_f$ does affect the accuracy of models, as shown in Figure~\ref{fig:accvsmf}.
As $M_f$ increases, the accuracy of the top model increases as well, albeit asymptotically.
This confirms that a larger feature space allows for more accurate results to a limit.
With 10 features per feature type, the odds of running into an instance without any of these is higher. 
As the feature space increases in size, more features can be considered when classifying a new instance, 
meaning more instances are likely to contain at least one feature to consider for classification.
These results merit the use of $M_f = 10000$ to generate and evaluate the final models.

The highest 5th percentile accuracy models for all $M_f$ values tested are consistently linear regressions.
This is a testament to the classifier's versatility with both small and large feature spaces.

The accuracies on the training data are consistently higher than those of the testing data (Figure~\ref{fig:accvsmf}).
As $M_f$ increases, the difference between the training and testing accuracies widens. 
This may be an indicator of overfitting in the data. 

Despite the risk of overfitting, 
the final models are trained with $M_f = 10000$ so as to maximise the result accuracy.
My theory is that using any larger value of $M_f$ will result in diminishing returns in testing accuracy, 
as the model starts to overfit the training data.

\subsection{The best classifier/parameter configurations}\label{sec:bestparams}

With the selected feature space $M_f = 10000$, the highest 5th percentile accuracies come from:
\begin{enumerate}
	\item the logistic regression classifier,
 	\item the linear support vector classifier (SVC) with regularization $C = 0.1$,
  	\item the multinomial naive Bayes classifier with laplace smoothing $\alpha = 1$ and without priors
\end{enumerate}
as per Section~\ref{sec:top3}.

The logistic regression relies on optimization, mapping instances to a multinomial class distribution.
This classifier scores the highest of all candidates, likely due to its optimization algorithm.
While optimization is slow (and not guaranteed to find the ideal fit), 
the model weights features proportionally to their importance sentiment classification.


The linear SVC also relies on optimization to find supports to generate lines separating the instances by sentiment.
It applies a one-vs-rest decision function, generating three lines separating each sentiment from the other two.
This makes the model effective at determining classes which are very different from the rest.
However, a weakness of the SVC is in distinguishing between classes with similar features. 
Furthermore, the model is highly reliant on a small subset of instance vectors to classify all other instances. 
While this simplicity reduces memory use, it can also abstract away useful distinctions for specific features.


The multinomial naive Bayes performs better than the Bernoulli naive Bayes classifier.
This result is not expected due to the expectation of binary features explained in Section~\ref{sec:vectorization}.
This naive Bayes model also performs best without fitting sentiment priors.
The consideration of priors is heavily reliant on whether the sentiment distribution in the \texttt{Test.csv} matches the \texttt{Train.csv}.
The naive Bayes classifier relies on simple laplace smoothing $\alpha = 1$. 
As $\alpha$ increases or decreases,
missing features become either too underrepresented or overrepresented (negatively affecting accuracy).

The Classifiers 1, 2, and 3 all perform better than the baseline model (Table 17). 
The only exception is the recall of the 0-R, in which the baseline model has the best results for some classes.
This is misleading since the 0-R baseline classifier labels all instances with the same sentiment, 
guaranteeing that one of the classes has no false negatives.


\subsection{The most sentiment-distribution-agnostic classifier}\label{sec:agnostic}

Since the \texttt{Test.csv} and \texttt{Train.csv} data are not guaranteed to follow the same sentiment distributions,
the true best model is selected by comparing the evaluation metrics over different train/test distribution splits.
As shown in Section~\ref{sec:4metrics}, the sentiment distribution of the training and testing data influences the evaluation metrics of classifiers.

Based on the accuracies only, training on a uniform sentiment distribution is preferable.
However, certain models may perform better for certain sentiments.
Since neutral sentiments are most prominent in the training set, models that do not consider the prior sentiment distributions 
(either due to a parameter or due to the training set used) generally yield higher neutral precisions than recalls,
indicating more false non-neutral than false neutral classifications.

For most applications of sentiment analysis the priority is to identify non-neutral instances.
Therefore, high precisions and recalls for non-neutral labels are prioritised.
This can be simplified into finding the models which generate the highest non-neutral $F_1$ scores.
Comparing Tables 15, 16, and 17, 
the linear SVC and logistic regression have the best $F_1$ scores in these sentiments.
Specifically the best scores are generated on uniform training sets, where both models perform similarly in precision and recall.
Therefore, either the linear SVC or the logistic regression can reliably be used on the \texttt{Test.csv} data,
given they are trained on a maximal subset of the training data with uniform sentiment distribution.

\pagebreak
\section{Conclusions}
% Conclusion: Demonstrate your identified knowledge about the problem.

Performing accurate sentiment analysis on tweets is difficult.
Vectorization of the features yields a large feature space, 
which contributes to the time and space complexity in training classifiers and poses the risk of overfitting results.
While minimizing the feature space reduces model training complexity, it also sacrifices the accuracy of classifiers.

Testing a multiple classifier combinations, 
the most accurate classifiers by estimated 5th percentile are the logistic regression, 
linear SVC ($C=0.1$), and multinomial naive Bayes ($\alpha=1$ and without priors).
With little information on the \texttt{Test.csv} data, 
the chosen models need to be highly accurate regardless of distributions of the testing or training sets.
Both the linear SVC and logistic regression can be used classify the \texttt{Test.csv} instances.
However, given the disadvantages of the linear SVC's reliance on a one-versus-rest decision function,
\textbf{I choose to generate the final predictions on a logistic regression trained on a maximals subset of instances with uniform sentiment distribution.}
\vfill
\pagebreak
\bibliographystyle{acl}
\bibliography{citations}

% \pagebreak
% \listoftables
% \section{Appendix: Evaluation of top classifiers for $M_f = 10000$}\label{sec:appa}

% \subsection{Results for Classifier 1}\label{sec:appc1}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & Bernoulli N.B. \\
% 			Include Priors & Yes \\
% 			$\alpha$ Smoothing & 1 \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.62 \\
% 			Precisions (+, n, --)	& 0.65, 0.55, 0.65 \\
% 			Recalls (+, n, --) 		& 0.70, 0.44, 0.74 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.49, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.60 \\
% 			Precisions (+, n, --) 	& 0.78, 0.60, 0.50 \\
% 			Recalls (+, n, --) 		& 0.14, 0.99, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.23, 0.74, 0.01 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.40 \\
% 			Precisions (+, n, --) 	& 0.94, 0.36, 0.90 \\
% 			Recalls (+, n, --) 		& 0.19, 0.99, 0.01 \\
% 			$F_1$ Scores (+, n, --) & 0.31, 0.52, 0.02 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.64 \\
% 			Precisions (+, n, --) 	& 0.56, 0.87, 0.48 \\
% 			Recalls (+, n, --) 		& 0.82, 0.50, 0.85 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.64, 0.62 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifier 1 metrics}
% 		\label{tbl:metrics-1st10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-1st-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-1st-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-1st-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Classifier 1's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-1st-ur}
% % \end{figure} 

% \pagebreak
% \subsection{Results for Classifier 2}\label{sec:appc2}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & Bernoulli N.B. \\
% 			Include Priors & No \\
% 			$\alpha$ Smoothing & 1 \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.62 \\
% 			Precisions (+, n, --) 	& 0.65, 0.55, 0.65 \\
% 			Recalls (+, n, --) 		& 0.70, 0.44, 0.74 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.49, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.61 \\
% 			Precisions (+, n, --) 	& 0.73, 0.60, 0.60 \\
% 			Recalls (+, n, --) 		& 0.16, 0.98, 0.01 \\
% 			$F_1$ Scores (+, n, --) & 0.26, 0.74, 0.02 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.42 \\
% 			Precisions (+, n, --) 	& 0.93, 0.36, 0.95 \\
% 			Recalls (+, n, --) 		& 0.23, 0.99, 0.03 \\
% 			$F_1$ Scores (+, n, --) & 0.37, 0.53, 0.05 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.64 \\
% 			Precisions (+, n, --) 	& 0.56, 0.87, 0.48 \\
% 			Recalls (+, n, --) 		& 0.82, 0.50, 0.85 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.64, 0.62 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifer 2 metrics}
% 		\label{tbl:metrics-2nd10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Uniform-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-2nd-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-Given-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-2nd-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-2nd-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/BernoulliNaiveBayesalpha1fit_priorFalse-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Classifier 2's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-2nd-ur}
% % \end{figure} 

% \pagebreak
% \subsection{Results for Classifier 3}\label{sec:appc3}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & Multinomial N.B. \\
% 			Include Priors & Yes \\
% 			$\alpha$ Smoothing & 1 \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.62 \\
% 			Precisions (+, n, --) 	& 0.67, 0.55, 0.62 \\
% 			Recalls (+, n, --) 		& 0.67, 0.41, 0.78 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.47, 0.69 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.61 \\
% 			Precisions (+, n, --) 	& 0.79, 0.60, 0.20 \\
% 			Recalls (+, n, --) 		& 0.17, 0.98, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.28, 0.74, 0.01 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.42 \\
% 			Precisions (+, n, --) 	& 0.94, 0.36, 1.00 \\
% 			Recalls (+, n, --) 		& 0.26, 0.99, 0.02 \\
% 			$F_1$ Scores (+, n, --) & 0.41, 0.53, 0.04 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.63 \\
% 			Precisions (+, n, --) 	& 0.58, 0.88, 0.45 \\
% 			Recalls (+, n, --) 		& 0.79, 0.48, 0.89 \\
% 			$F_1$ Scores (+, n, --) & 0.67, 0.62, 0.60 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Classifer 3 metrics}
% 		\label{tbl:metrics-3rd10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Uniform-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-3rd-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-Given-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-3rd-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-3rd-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MultinomialNaiveBayesalpha1fit_priorTrue-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Classifier 3's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-3rd-ur}
% % \end{figure}

% \pagebreak
% \subsection{Results for the baseline model (Classifier 4)}\label{sec:appc4}

% \begin{table}[h]
% 	\begin{center}
% 		\begin{tabular}{|l|l|}			
% 			\hline
% 			Parameter & Value \\
% 			\hline
% 			Classifier & 0-R \\
% 			\hline\hline
% 			Uniform Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.33 \\
% 			Precisions (+, n, --) 	& 0.00, 0.00, 0.33 \\
% 			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.50 \\
% 			\hline\hline
% 			Given Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.58 \\
% 			Precisions (+, n, --) 	& 0.00, 0.58, 0.00 \\
% 			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.73, 0.00 \\
% 			\hline\hline
% 			Given Training & Uniform Testing \\
% 			\hline
% 			Accuracy & 0.33 \\
% 			Precisions (+, n, --) 	& 0.00, 0.33, 0.00 \\
% 			Recalls (+, n, --) 		& 0.00, 1.00, 0.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.50, 0.00 \\
% 			\hline\hline
% 			Uniform Training & Given Testing \\
% 			\hline
% 			Accuracy & 0.17 \\
% 			Precisions (+, n, --) 	& 0.00, 0.00, 0.17 \\
% 			Recalls (+, n, --) 		& 0.00, 0.00, 1.00 \\
% 			$F_1$ Scores (+, n, --) & 0.00, 0.00, 0.29 \\
% 			\hline
% 		\end{tabular}
% 		\caption{Baseline classifier (Classifier 4) metrics}
% 		\label{tbl:metrics-base10000}
% 	\end{center}
% \end{table}

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Uniform-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for uniform training and uniform testing data}
% % 	\label{fig:cf-base-uu}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-Given-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for given training and given testing data}
% % 	\label{fig:cf-base-rr}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-GivenTrainingandUniformTesting-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for given training and uniform testing data}
% % 	\label{fig:cf-base-ru}
% % \end{figure} 

% % \begin{figure}[H]
% % 	\centering
% % 	\includegraphics[width = 0.4\textwidth]{cf/MostFrequentClass0R-UniformTrainingandGivenTesting-confusion-matrix.png}
% % 	\caption{Baseline classifier's confusion matrix for uniform training and given testing data}
% % 	\label{fig:cf-base-ur}
% % \end{figure} 

\end{document}
