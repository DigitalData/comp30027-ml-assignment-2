{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets | Model\n",
    "\n",
    "This notebook contains code focused on selecting features for a best accuracy model.\n",
    "\n",
    "**Full Name:** `Xavier Travers`\n",
    "\n",
    "**Student ID:** `1178369`\n",
    "\n",
    "First, import the data and the necessary python modules for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Xavier\n",
      "[nltk_data]     Travers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from scipy import sparse\n",
    "\n",
    "# import the feature extractors\n",
    "from features import *\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# classifiers\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# pipeline and parameter optimisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "\n",
    "# import the data\n",
    "train_data = pd.read_csv(\"../datasets/Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"../datasets/Test.csv\", sep=',')\n",
    "\n",
    "# extract the training inputs and outputs\n",
    "train_input = train_data[['text']].values[:, 0]\n",
    "train_output = train_data[['sentiment']].values[:, 0]\n",
    "test_input = test_data[['text']].values[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Container Object\n",
    "This will contain classifiers (which can then be compared by `GridSearchCV`).\n",
    "\n",
    "Reference: https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierContainer(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        estimator = LogisticRegression(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "        :param estimator: sklearn object - The classifier\n",
    "        \"\"\" \n",
    "\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)\n",
    "\n",
    "    def cv_scores(self, X, y, cv=10): # returns a dictionary of average score and scores list\n",
    "        scores = cross_val_score(self.estimator, X, y, cv = cv)\n",
    "        return {\n",
    "            'values': scores,\n",
    "            'average': np.average(scores)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "The baseline model for this dataset will be `0R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average': 0.5806348194793232,\n",
      " 'values': array([0.58046768, 0.58046768, 0.58073394, 0.58073394, 0.58073394,\n",
      "       0.58073394, 0.58073394, 0.58073394, 0.58073394, 0.58027523])}\n"
     ]
    }
   ],
   "source": [
    "# create the baseline model\n",
    "baseline = ClassifierContainer(DummyClassifier(strategy=\"most_frequent\"))\n",
    "baseline.fit(train_input, train_output)\n",
    "\n",
    "# Check the baseline model's accuracy\n",
    "pprint(baseline.cv_scores(train_input, train_output, cv = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "The features can be aggregated or added one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21802, 5000)\n",
      "(21802, 1000)\n",
      "(21802, 1000)\n",
      "(21802, 4)\n",
      "(21802, 72)\n",
      "(21802, 7076)\n",
      "<class 'scipy.sparse._coo.coo_matrix'>\n",
      "(6099, 5000)\n",
      "(6099, 1000)\n",
      "(6099, 1000)\n",
      "(6099, 4)\n",
      "(6099, 72)\n",
      "(6099, 7076)\n",
      "<class 'scipy.sparse._coo.coo_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# The number of top TweetFeatures to filter through\n",
    "BOW_MAX_FEATURES = 5000\n",
    "TFIDF_MAX_FEATURES = BOW_MAX_FEATURES\n",
    "MAX_FEATURES = 1000\n",
    "\n",
    "# create the transformers\n",
    "transformers = [\n",
    "    TweetBagOfWords(max_features=BOW_MAX_FEATURES), # Bag of words vectorizer\n",
    "    # TweetTFIDF(max_features=TFIDF_MAX_FEATURES), # TF-IDF vectorizer\n",
    "    # TweetMetrics(), # Count metrics for tweets\n",
    "    # TweetWordLengths(max_features=MAX_FEATURES), # Word lengths\n",
    "    # TweetCharacterFrequencies(max_features=MAX_FEATURES, alphabetic=False), # Character distributions\n",
    "    # TweetLinks(max_features=MAX_FEATURES), # Links\n",
    "    TweetHashtags(max_features=MAX_FEATURES), # Hashtags\n",
    "    TweetReferences(max_features=MAX_FEATURES), # References\n",
    "    TweetEmoticons(), # Emoticons\n",
    "    TweetPhonetics(max_features=MAX_FEATURES), # Raw phonetics\n",
    "    # TweetPoetics(), # Poetic Phonetics\n",
    "]\n",
    "\n",
    "# perform the transforms and combine them into the input matrix\n",
    "train_transforms = [tf.fit_transform(train_input, train_output) for tf in transformers]\n",
    "\n",
    "# print the shapes of the transforms\n",
    "for t in train_transforms:\n",
    "    print(t.shape)\n",
    "\n",
    "train_features = None\n",
    "if len(train_transforms) == 1:\n",
    "    train_features = train_transforms[0]\n",
    "else:\n",
    "    train_features = sparse.hstack(train_transforms)\n",
    "\n",
    "# feature matrix information\n",
    "print(train_features.shape)\n",
    "print(type(train_features))\n",
    "\n",
    "# do the same to test data\n",
    "test_transforms = [tf.transform(test_input) for tf in transformers]\n",
    "\n",
    "# print the shapes of the transforms\n",
    "for t in test_transforms:\n",
    "    print(t.shape)\n",
    "\n",
    "test_features = None\n",
    "if len(test_transforms) == 1:\n",
    "    test_features = test_transforms[0]\n",
    "else:\n",
    "    test_features = sparse.hstack(test_transforms)\n",
    "\n",
    "# feature matrix information\n",
    "print(test_features.shape)\n",
    "print(type(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mnb score: 0.612\n",
      "[[0.62952774 0.6258597  0.6353211  0.61055046 0.61651376 0.62155963\n",
      " 0.61880734 0.56743119 0.58899083 0.60366972]]\n",
      "Average bnb score: 0.621\n",
      "[[0.63548831 0.63594681 0.64770642 0.62522936 0.6233945  0.63256881\n",
      " 0.64724771 0.5587156  0.59678899 0.60917431]]\n",
      "['negative' 'negative' 'neutral' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB(fit_prior=True)\n",
    "bnb = BernoulliNB(fit_prior=True)\n",
    "\n",
    "nb_classifiers = {'mnb': mnb, 'bnb': bnb}\n",
    "nb_accs = {}\n",
    "nb_avgs = {}\n",
    "\n",
    "for nb in nb_classifiers.keys():\n",
    "    accs = cross_val_score(nb_classifiers[nb], train_features, train_output, cv = 10)\n",
    "    avg = np.mean(accs)\n",
    "    print(f\"Average {nb} score: {avg:.3f}\")\n",
    "    print(f\"[{accs}]\")\n",
    "    nb_accs[nb] = accs\n",
    "    nb_avgs[nb] = avg\n",
    "\n",
    "# print('Avg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "# print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "# print('Avg BNB score: {}'.format(np.mean(bnb_accs)))\n",
    "\n",
    "bnb.fit(train_features, train_output)\n",
    "y_pred = bnb.predict(test_features)\n",
    "print(y_pred[:5])\n",
    "predictions1 = pd.DataFrame({'id': test_data['id'], 'sentiment': y_pred})\n",
    "predictions1.to_csv('./pred-bnb-fp-1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Explorations\n",
    "Here, different models are cross-validated to find the best one.\n",
    "\n",
    "References: \n",
    "- https://scikit-learn.org/stable/modules/compose.html#pipeline\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['norm', 'clf']\n",
      "parameters:\n",
      "[{'clf__estimator': [LinearSVC()],\n",
      "  'clf__estimator__C': [10, 1, 0.001, 1e-06],\n",
      "  'clf__estimator__max_iter': [1000],\n",
      "  'norm': [StandardScaler(with_mean=False)]},\n",
      " {'clf__estimator': [SVC()],\n",
      "  'clf__estimator__C': [10, 1, 0.001, 1e-06],\n",
      "  'clf__estimator__kernel': ['rbf', 'sigmoid'],\n",
      "  'clf__estimator__max_iter': [1000],\n",
      "  'norm': [StandardScaler(with_mean=False)]},\n",
      " {'clf__estimator': [SVC()],\n",
      "  'clf__estimator__C': [10, 1, 0.001, 1e-06],\n",
      "  'clf__estimator__degree': [3, 5, 10],\n",
      "  'clf__estimator__kernel': ['poly'],\n",
      "  'clf__estimator__max_iter': [1000],\n",
      "  'norm': [StandardScaler(with_mean=False)]},\n",
      " {'clf__estimator': [KNeighborsClassifier()],\n",
      "  'clf__estimator__n_neighbors': [5, 10, 100, 1000, 5000]},\n",
      " {'clf__estimator': [DecisionTreeClassifier()],\n",
      "  'clf__estimator__criterion': ['gini', 'entropy'],\n",
      "  'clf__estimator__max_depth': [None, 1, 10, 1000]},\n",
      " {'clf__estimator': [MultinomialNB(), BernoulliNB()],\n",
      "  'clf__estimator__alpha': [1e-08, 1e-06, 0.0001, 0.01, 1, 100, 10000],\n",
      "  'clf__estimator__fit_prior': [True, False]},\n",
      " {'clf__estimator': [LogisticRegression()],\n",
      "  'clf__estimator__C': [1, 2, 4, 8],\n",
      "  'clf__estimator__max_iter': [1000],\n",
      "  'clf__estimator__solver': ['newton-cg', 'sag', 'saga', 'lbfgs']}]\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "done in 440.895s\n",
      "\n",
      "Best score: 0.665\n",
      "Best parameters set:\n",
      "{'clf': ClassifierContainer(estimator=LogisticRegression(C=1, max_iter=1000,\n",
      "                                                 solver='saga')),\n",
      " 'clf__estimator': LogisticRegression(C=1, max_iter=1000, solver='saga'),\n",
      " 'clf__estimator__C': 1,\n",
      " 'clf__estimator__class_weight': None,\n",
      " 'clf__estimator__dual': False,\n",
      " 'clf__estimator__fit_intercept': True,\n",
      " 'clf__estimator__intercept_scaling': 1,\n",
      " 'clf__estimator__l1_ratio': None,\n",
      " 'clf__estimator__max_iter': 1000,\n",
      " 'clf__estimator__multi_class': 'auto',\n",
      " 'clf__estimator__n_jobs': None,\n",
      " 'clf__estimator__penalty': 'l2',\n",
      " 'clf__estimator__random_state': None,\n",
      " 'clf__estimator__solver': 'saga',\n",
      " 'clf__estimator__tol': 0.0001,\n",
      " 'clf__estimator__verbose': 0,\n",
      " 'clf__estimator__warm_start': False,\n",
      " 'memory': 'C:\\\\Users\\\\XAVIER~1\\\\AppData\\\\Local\\\\Temp\\\\tmp1f9mjdr_',\n",
      " 'norm': 'passthrough',\n",
      " 'steps': [('norm', 'passthrough'),\n",
      "           ('clf',\n",
      "            ClassifierContainer(estimator=LogisticRegression(C=1, max_iter=1000,\n",
      "                                                 solver='saga')))],\n",
      " 'verbose': False}\n",
      "['neutral' 'neutral' 'neutral' 'neutral' 'positive']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate variable choices for the model\n",
    "MAX_ITER = [1000]\n",
    "SVC_C = [10, 1, 1e-3, 1e-6]\n",
    "SVC_KERNEL = ['rbf', 'sigmoid']\n",
    "SVC_POLY_DEG = [3, 5, 10]\n",
    "KNN_N = [5, 10, 100, 1000, 5000]\n",
    "DT_CRITERION = ['gini', 'entropy']\n",
    "DT_MAX_DEPTH = [None, 1, 10, 1000]\n",
    "NB_ALPHAS = [10**pw for pw in range(-8, 6, 2)]\n",
    "NB_FIT_PRIOR = [True, False]\n",
    "LR_SOLVER = ['newton-cg', 'sag', 'saga', 'lbfgs']\n",
    "LR_C = [2**pw for pw in range(0, 4)]\n",
    "\n",
    "# define the pipeline for parameter selection\n",
    "cachedir = mkdtemp() \n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm', 'passthrough'),\n",
    "        ('clf', ClassifierContainer()), # allows switching between classifiers\n",
    "    ],\n",
    "    memory=cachedir\n",
    ")\n",
    "\n",
    "# add the parameters possible (to permutate over)\n",
    "parameters = [\n",
    "    { # CLF: Linear Support Vector Classifiers\n",
    "        'norm': [StandardScaler(with_mean=False)],\n",
    "        'clf__estimator': [LinearSVC()],\n",
    "        'clf__estimator__C': SVC_C,\n",
    "        'clf__estimator__max_iter': MAX_ITER,\n",
    "    },\n",
    "    { # CLF: Support Vector Classifiers (no poly here since it's check below)\n",
    "        'norm': [StandardScaler(with_mean=False)],\n",
    "        'clf__estimator': [SVC()],\n",
    "        'clf__estimator__kernel': SVC_KERNEL,\n",
    "        'clf__estimator__C': SVC_C,\n",
    "        'clf__estimator__max_iter': MAX_ITER,\n",
    "    },\n",
    "    { # CLF: Support Vector Polynomial Classifiers (iterates over degrees)\n",
    "        'norm': [StandardScaler(with_mean=False)],\n",
    "        'clf__estimator': [SVC()],\n",
    "        'clf__estimator__kernel': ['poly'],\n",
    "        'clf__estimator__degree': SVC_POLY_DEG,\n",
    "        'clf__estimator__C': SVC_C,\n",
    "        'clf__estimator__max_iter': MAX_ITER,\n",
    "    },\n",
    "    { # CLF: K Nearest-Neighbours\n",
    "        'clf__estimator': [KNeighborsClassifier()],\n",
    "        'clf__estimator__n_neighbors': KNN_N,\n",
    "    },\n",
    "    { # CLF: Decision Trees\n",
    "        'clf__estimator': [DecisionTreeClassifier()],\n",
    "        'clf__estimator__criterion': DT_CRITERION,\n",
    "        'clf__estimator__max_depth': DT_MAX_DEPTH,\n",
    "    },\n",
    "    { # CLF: Multinomial and Bernoulli Naive Bayes\n",
    "        'clf__estimator': [MultinomialNB(), BernoulliNB()],\n",
    "        'clf__estimator__alpha': NB_ALPHAS,\n",
    "        'clf__estimator__fit_prior': NB_FIT_PRIOR,\n",
    "    },\n",
    "    { # CLF: Logistic Regression\n",
    "        'clf__estimator': [LogisticRegression()],\n",
    "        'clf__estimator__C': LR_C,\n",
    "        'clf__estimator__max_iter': MAX_ITER,\n",
    "        'clf__estimator__solver': LR_SOLVER,\n",
    "    },\n",
    "]\n",
    "\n",
    "# this next code needs to occur in a main block\n",
    "if __name__ == \"__main__\":    \n",
    "    # find the best classifier by grid search\n",
    "    verbosity = 1\n",
    "    grid_search = GridSearchCV(pipeline, parameters, \n",
    "        cv=StratifiedKFold(random_state=101, shuffle=True), \n",
    "        verbose=verbosity,\n",
    "        n_jobs = -1)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_features, train_output)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    pprint(best_parameters)\n",
    "\n",
    "    y_pred = grid_search.predict(test_features)\n",
    "    print(y_pred[:5])\n",
    "    predictions1 = pd.DataFrame({'id': test_data['id'], 'sentiment': y_pred})\n",
    "    predictions1.to_csv('./pred-2.csv', index = False)\n",
    "\n",
    "rmtree(cachedir)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
